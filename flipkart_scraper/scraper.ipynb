{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pillow in /home/abhenayshree/.local/lib/python3.10/site-packages (10.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/abhenayshree/.local/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (2.25.1)\n",
      "Requirement already satisfied: pandas in /home/abhenayshree/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/abhenayshree/.local/lib/python3.10/site-packages (from pytesseract) (24.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/abhenayshree/.local/lib/python3.10/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/abhenayshree/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/abhenayshree/.local/lib/python3.10/site-packages (from pandas) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/abhenayshree/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract pillow beautifulsoup4 requests pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd=r'/usr/local/bin/tesseract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "# Function to scrape Flipkart based on the category URL\n",
    "def scrape_flipkart(url, category):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = []\n",
    "\n",
    "    # Modify the below class names and tags according to the actual structure of the Flipkart page.\n",
    "    product_divs = soup.find_all('div', class_='_1AtVbE')  # Example class for product containers\n",
    "\n",
    "    for product in product_divs:\n",
    "        try:\n",
    "            product_name = product.find('a', class_='IRpwTa').text  # Example for product name\n",
    "            product_price = product.find('div', class_='_30jeq3').text  # Example for product price\n",
    "            product_rating = product.find('div', class_='_3LWZlK').text  # Example for product rating\n",
    "            image_url = product.find('img', class_='_396cs4')['src']  # Example for product image URL\n",
    "            \n",
    "            # Extract text from image (OCR)\n",
    "            ocr_text = extract_text_from_image(image_url)\n",
    "            \n",
    "            products.append({\n",
    "                'Category': category,\n",
    "                'Product Name': product_name,\n",
    "                'Price': product_price,\n",
    "                'Rating': product_rating,\n",
    "                'Image URL': image_url,\n",
    "                'Extracted Text': ocr_text\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # Skip any product without complete details\n",
    "            print(f\"Skipping a product due to missing data: {e}\")\n",
    "    \n",
    "    return products\n",
    "\n",
    "# Common details to be extracted for all categories\n",
    "def extract_common_data(product):\n",
    "    return {\n",
    "        'Product Name': product.get('Product Name', ''),\n",
    "        'Price': product.get('Price', ''),\n",
    "        'Rating': product.get('Rating', ''),\n",
    "        'Image URL': product.get('Image URL', ''),\n",
    "        'Extracted Text (OCR)': product.get('Extracted Text', '')\n",
    "    }\n",
    "\n",
    "# Category-specific data extraction (example for 'Fashion & Apparel')\n",
    "def extract_fashion_data(product):\n",
    "    # Add any extra details you want specifically for this category\n",
    "    return {\n",
    "        'Material': 'To be extracted',  # Add real extraction logic based on available details\n",
    "        'Brand': 'To be extracted'\n",
    "    }\n",
    "\n",
    "# Category-specific data extraction (example for 'Electronics')\n",
    "def extract_electronics_data(product):\n",
    "    return {\n",
    "        'Warranty': 'To be extracted',  # Add extraction logic based on available details\n",
    "        'Specifications': 'To be extracted'\n",
    "    }\n",
    "\n",
    "# Function to save scraped data into a CSV\n",
    "def save_to_csv(data, file_name):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "category_urls={\n",
    "    'Fashion & Apparel':'https://www.flipkart.com/clothing-and-accessories/pr?sid=clo&otracker=categorytree&p%5B%5D=facets.ideal_for%255B%255D%3DMen&otracker=nmenu_sub_Men_0_Clothing',\n",
    "    'Electronics': 'https://www.flipkart.com/mobile-phones-store?otracker=nmenu_sub_Electronics_0_Mobiles',\n",
    "    'Beauty Products':'https://www.flipkart.com/beauty-and-grooming/pr?sid=g9b&p[]=facets.serviceability%5B%5D%3Dtrue&otracker=categorytree&otracker=nmenu_sub_Women_0_Beauty%20%26%20Grooming',  \n",
    "}\n",
    "\n",
    "def scrape_all_categories():\n",
    "    all_data = []\n",
    "    for category, url in category_urls.items():\n",
    "        print(f\"Scraping category: {category}\")\n",
    "        products = scrape_flipkart(url, category)\n",
    "        \n",
    "        for product in products:\n",
    "            common_data = extract_common_data(product)\n",
    "            if category == 'Fashion & Apparel':\n",
    "                category_data = extract_fashion_data(product)\n",
    "            elif category == 'Electronics':\n",
    "                category_data = extract_electronics_data(product)\n",
    "            # Add more categories with specific extraction logic as needed\n",
    "            else:\n",
    "                category_data = {}\n",
    "\n",
    "            combined_data = {**common_data, **category_data}\n",
    "            all_data.append(combined_data)\n",
    "    \n",
    "    # Save all scraped data to a CSV file\n",
    "    save_to_csv(all_data, 'flipkart_products_data.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
